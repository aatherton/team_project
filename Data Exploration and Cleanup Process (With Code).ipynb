{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Data Exploration and Cleanup Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing Dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import time\n",
    "\n",
    "#load your API keys\n",
    "file_name =\"C:\\\\Users\\\\spaug\\\\Python\\\\KU_Coding\\\\kubootcamp0\\\\api_keys.json\"\n",
    "data = json.load(open(file_name))\n",
    "o_key = 'vvr6c9yegfkzkudyzvvg5za8'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started by seeing what the JSON data looked like, and \n",
    "grabbed a unique identifier for each NFL team. We exported  this list into 'teams_df.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up URL and API call for team_id\n",
    "url = \"http://api.sportradar.us/nfl-ot2/seasontd/\"\n",
    "season = \"REG\"\n",
    "year = \"2017\"\n",
    "output_format = \"json\"\n",
    "team_id = \"33405046-04ee-4058-a950-d606f8c30852\"\n",
    "\n",
    "query_url = url + year + \"/\" + season + \"/teams/\" + team_id + \"/statistics.\" + output_format + \"?api_key=\" + o_key\n",
    "nfl_response = requests.get(query_url)\n",
    "nfl_json = nfl_response.json()\n",
    "\n",
    "team_id_url = \"http://api.sportradar.us/nfl-ot2/league/hierarchy.json?api_key=\"\n",
    "\n",
    "team_id_response = requests.get(team_id_url + o_key)\n",
    "team_id_json = team_id_response.json()\n",
    "\n",
    "# Setting up teams_df variables\n",
    "conf_count = 0\n",
    "conferences = []\n",
    "divisions = []\n",
    "teams = []\n",
    "team_id = []\n",
    "venue_name = []\n",
    "venue_city = []\n",
    "venue_state = []\n",
    "venue_capacity = []\n",
    "venue_surface = []\n",
    "venue_roof_type = []\n",
    "\n",
    "# Starting conference gathering loop\n",
    "for conference in team_id_json['conferences']:\n",
    "\n",
    "    # Declaring division counter inside so that it resets for each iteration\n",
    "    div_count = 0\n",
    "    \n",
    "    # Starting division gathering loop\n",
    "    for division in team_id_json['conferences'][conf_count]:\n",
    "        \n",
    "        # Declaring teams counter inside so that it resets for each iteration\n",
    "        team_count = 0\n",
    "        \n",
    "        # Starting team gathering loop\n",
    "        for team in team_id_json['conferences'][conf_count]['divisions'][div_count]:\n",
    "\n",
    "            # Creating variables for easier handling when appending\n",
    "            temp_name = team_id_json['conferences'][conf_count]['divisions'][div_count]['teams'][team_count]['name']\n",
    "            temp_market = team_id_json['conferences'][conf_count]['divisions'][div_count]['teams'][team_count]['market']\n",
    "            \n",
    "            # Appending conferences, divisions, team name and team IDs to lists\n",
    "            conferences.append(team_id_json['conferences'][conf_count]['name'])\n",
    "            divisions.append(team_id_json['conferences'][conf_count]['divisions'][div_count]['name'])\n",
    "            teams.append((str(temp_market) + \" \" + str(temp_name)))\n",
    "            team_id.append(team_id_json['conferences'][conf_count]['divisions'][div_count]['teams'][team_count]['id'])\n",
    "            venue_name.append(team_id_json['conferences'][conf_count]['divisions'][div_count]['teams'][team_count]['venue']['name'])\n",
    "            venue_city.append(team_id_json['conferences'][conf_count]['divisions'][div_count]['teams'][team_count]['venue']['city'])\n",
    "            venue_state.append(team_id_json['conferences'][conf_count]['divisions'][div_count]['teams'][team_count]['venue']['state'])\n",
    "            venue_capacity.append(team_id_json['conferences'][conf_count]['divisions'][div_count]['teams'][team_count]['venue']['capacity'])\n",
    "            venue_surface.append(team_id_json['conferences'][conf_count]['divisions'][div_count]['teams'][team_count]['venue']['surface'])\n",
    "            venue_roof_type.append(team_id_json['conferences'][conf_count]['divisions'][div_count]['teams'][team_count]['venue']['roof_type'])\n",
    "            \n",
    "            # Incrementing teams counter\n",
    "            team_count += 1\n",
    "            \n",
    "        # Incrementing division counter\n",
    "        div_count += 1\n",
    "    \n",
    "    # Incrementing conference counter\n",
    "    conf_count += 1\n",
    "    \n",
    "\n",
    "teams_df = pd.DataFrame(list(zip(teams, conferences, divisions, venue_name, venue_city, venue_state, venue_capacity, \\\n",
    "                                 venue_surface, venue_roof_type, team_id)), \\\n",
    "                                 columns=['Team', 'Conference', 'Division', 'Venue Name', 'Venue City', \\\n",
    "                                          'Venue State', 'Venue Capacity', 'Playing Surface', 'Stadium Type', 'Team ID'])\n",
    "\n",
    "#export to teams_df, this file contains data on each NFL team and stadium, we used this to get TEAM_IDs\n",
    "teams_df.to_csv('teams_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then looped through each team in teams_df, and retrieved seasonal stats \n",
    "for that team from each year the service had data from (2000-2017). \n",
    "We appended each teams data to a large data frame we called 'all_players.csv'. \n",
    "Here is where we ran into our first problem - the seasonal stats we downloaded \n",
    "did not include colleges for each player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_teams_dict = {}\n",
    "#loop through each teams data\n",
    "# change range above for desired # of teams, to get full list use\n",
    "for x in range(0, 33):\n",
    "    team_id = teams_df['Team ID'][x]\n",
    "    print(teams_df['Team'][x])\n",
    "    team_dict = {}\n",
    "    \n",
    "#     loop throug each years data\n",
    "    for y in range(0, 18):\n",
    "#       change range above for desired of years, where 0,17 is the max range (2000 - 2017)\n",
    "        year = y + 2000\n",
    "        print(year)\n",
    "        team_season_url = 'http://api.sportradar.us/nfl-ot2/seasontd/' + str(year) + '/reg/teams/' + team_id + '/statistics.json?api_key=' + o_key\n",
    "        team_season_response = requests.get(team_season_url)\n",
    "        time.sleep(1)\n",
    "        response = team_season_response.json()\n",
    "        team_dict[y] = response\n",
    "    all_teams_dict[x] = team_dict\n",
    "    \n",
    "master_id = []\n",
    "position = []\n",
    "games_played = []\n",
    "name = []\n",
    "p_yards = []\n",
    "p_td = []\n",
    "p_int = []\n",
    "ru_yards = []\n",
    "ru_td = []\n",
    "fumbles = []\n",
    "rec_yards = []\n",
    "rec_td = []\n",
    "rec_rec = []\n",
    "kick_ret = []\n",
    "punt_ret = []\n",
    "year_list = []\n",
    "\n",
    "for x in all_teams_dict:\n",
    "    for y in all_teams_dict[x]:\n",
    "        for z in range(0,len(all_teams_dict[x][y]['players'])):\n",
    "            try:\n",
    "                master_id.append(all_teams_dict[x][y]['players'][z]['id'] + '_' + all_teams_dict[x][y]['id'])\n",
    "            except KeyError:\n",
    "                master_id.append(0)\n",
    "            try:\n",
    "                year_list.append(all_teams_dict[x][y]['season']['year'])\n",
    "            except KeyError:\n",
    "                year_list.append(0)\n",
    "            try:    \n",
    "                games_played.append(all_teams_dict[x][y]['players'][z]['games_played'])\n",
    "            except KeyError:\n",
    "                games_played.append(0)\n",
    "            try:\n",
    "                name.append(all_teams_dict[x][y]['players'][z]['name'])\n",
    "            except KeyError:\n",
    "                name.append(0)\n",
    "            try:\n",
    "                position.append(all_teams_dict[x][y]['players'][z]['position'])\n",
    "            except KeyError:\n",
    "                position.append(0)\n",
    "            try:\n",
    "                p_yards.append(all_teams_dict[x][y]['players'][z]['passing']['yards']  )\n",
    "            except KeyError:\n",
    "                p_yards.append(0)\n",
    "            try:\n",
    "                p_td.append(all_teams_dict[x][y]['players'][z]['passing']['touchdowns'])\n",
    "            except KeyError:\n",
    "                p_td.append(0)\n",
    "            try:\n",
    "                p_int.append(all_teams_dict[x][y]['players'][z]['passing']['interceptions'])\n",
    "            except KeyError:\n",
    "                p_int.append(0)\n",
    "            try:\n",
    "                ru_yards.append(all_teams_dict[x][y]['players'][z]['rushing']['yards'])\n",
    "            except KeyError:\n",
    "                ru_yards.append(0)\n",
    "            try:\n",
    "                ru_td.append(all_teams_dict[x][y]['players'][z]['rushing']['touchdowns']  )\n",
    "            except KeyError:\n",
    "                ru_td.append(0)\n",
    "            try:\n",
    "                fumbles.append(all_teams_dict[x][y]['players'][z]['fumbles']['fumbles']  )\n",
    "            except KeyError:\n",
    "                fumbles.append(0)\n",
    "            try:\n",
    "                rec_yards.append(all_teams_dict[x][y]['players'][z]['receiving']['yards']   )\n",
    "            except KeyError:\n",
    "                rec_yards.append(0)\n",
    "            try:\n",
    "                rec_td.append(all_teams_dict[x][y]['players'][z]['receiving']['touchdowns'] )\n",
    "            except KeyError:\n",
    "                rec_td.append(0)\n",
    "            try:\n",
    "                rec_rec.append(all_teams_dict[x][y]['players'][z]['receiving']['receptions'] )\n",
    "            except KeyError:\n",
    "                rec_rec.append(0)\n",
    "            try:\n",
    "                kick_ret.append(all_teams_dict[x][y]['players'][z]['kick_returns']['touchdowns'])\n",
    "            except KeyError:\n",
    "                kick_ret.append(0)\n",
    "            try:\n",
    "                punt_ret.append(all_teams_dict[x][y]['players'][z]['punt_returns']['touchdowns'])\n",
    "            except KeyError:\n",
    "                punt_ret.append(0)\n",
    "                \n",
    "players_df = pd.DataFrame(list(zip(master_id, name, year_list, position, \n",
    "                                    games_played, p_yards, p_td, p_int, ru_yards,\n",
    "                                    ru_td, fumbles, rec_yards, rec_td, rec_rec, kick_ret, punt_ret)), \\\n",
    "                          columns=['Master_Id','Name', 'Year', 'Position','Games_Played','P_Yards','P_TD', 'P_Int',\n",
    "                                 'Rush_Yards','Rush_TD','Fumbles','Rec_Yards','Rec_TD','Receptions',\n",
    "                                  'Kick_Returns', 'Punt_Returns'])\n",
    "\n",
    "players_df.to_csv('all_players.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remedy this, we had to filter the list of 35,000+ rows that contained multiple years of \n",
    "data for each player, and extract a list of unique player IDs. The total unique IDs \n",
    "were ~ 9,000, so we limited the list to offensive players only since they generate the most \n",
    "fantasy points. Surprisingly this was only about 2,600 records. We took this list and made a \n",
    "new call to a different Sportradar service for individual data. We then retrieved each college \n",
    "associated with a players uniqueID, and exported that to 'player_college.csv'. We did not obtain this file until late in the data collection process, so we adding it in was done last, and that part of the process is described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_name = 'all_players.csv'\n",
    "all_players_df = pd.read_csv(file_name)\n",
    "\n",
    "positions_list = ['QB','WR','TE','FB','RB']\n",
    "offense_df = all_players_df.loc[all_players_df['Position'].isin(positions_list)]\n",
    "offense_df1 = offense_df.reset_index()\n",
    "\n",
    "player_list = []\n",
    "team_list = []\n",
    "for x in range(0,len(offense_df1)):\n",
    "    player_list.append(offense_df1['Master_Id'].str.split('_')[x][0])\n",
    "    team_list.append(offense_df1['Master_Id'].str.split('_')[x][1])\n",
    "    print(str(x) + '/' + str(len(offense_df1)))\n",
    "    \n",
    "    \n",
    "player_df1 = pd.DataFrame(player_list)\n",
    "player_df1.columns = ['Player_Id']\n",
    "team_df1 = pd.DataFrame(team_list)\n",
    "team_df1.columns = ['Team_Id']\n",
    "offense_df2 = pd.concat([player_df1, team_df1, offense_df1], axis=1)\n",
    "offense_df3 = offense_df2.iloc[:,[0,1,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]]\n",
    "offense_df3.to_csv\n",
    "offense_df4 = offense_df3.drop_duplicates('Player_Id')\n",
    "offense_df3.to_csv('Offensive_Only.csv')\n",
    "\n",
    "offense_df5 = offense_df4.iloc[:,[0,2]]\n",
    "offense_df5 = offense_df5.reset_index()\n",
    "offense_df5 = offense_df5.iloc[:,[1,2]]\n",
    "offense_df5.to_csv('unique_players.csv')\n",
    "\n",
    "# Setting up API key pulls - Need 3 seperate keys to get around call limit\n",
    "api_dir = os.path.dirname(os.path.dirname(os.path.realpath('__file__')))\n",
    "file_name = os.path.join(api_dir, \"api_keys.json\")\n",
    "data = json.load(open(file_name))\n",
    "\n",
    "one_key = data['sportradar_nfl_official_key1']\n",
    "two_key = data['sportradar_nfl_official_key2']\n",
    "three_key = data['sportradar_nfl_official_key3']\n",
    "\n",
    "# print(ce_key, ez_key, jz_key)\n",
    "\n",
    "# Finding file with unique player names to search and then starting a loop to interate\n",
    "player_file = \"unique_players.csv\"\n",
    "with open(player_file, \"r\") as p_list:\n",
    "    # Reading in all the player data\n",
    "    player_df = pd.read_csv(p_list)\n",
    "\n",
    "save_path = \"unique_players.csv\"\n",
    "\n",
    "# Setting up URL and API call for team_id\n",
    "key = [one_key, two_key, three_key]\n",
    "counter = 0\n",
    "manip_count = 0\n",
    "college_df = []\n",
    "#player_df['Player_Id']\n",
    "\n",
    "for player in player_df['Player_Id']:\n",
    "    # Building url and calling api\n",
    "    url = f\"https://api.sportradar.us/nfl-ot2/players/{player}/profile.json?api_key=\" + str(key[counter])\n",
    "\n",
    "    # Calling API - counts against API limit\n",
    "    api_response = requests.get(url)\n",
    "    \n",
    "    # Hard coding in a 1 call per second limit - api server will reject anything faster \n",
    "    time.sleep(1.1)\n",
    "    \n",
    "    # Reading json\n",
    "    api_json = api_response.json()\n",
    "    \n",
    "    # Counters - cycling keys every 900 calls so as to not hit a limit on any key\n",
    "    manip_count += 1\n",
    "    counter = manip_count % 3\n",
    "    \n",
    "    # Appending college, N/A if nothing\n",
    "    try:\n",
    "        college_df.append(api_json['college'])\n",
    "        print(str(manip_count) + \" - \" + str(api_json['college']))\n",
    "    except:\n",
    "        college_df.append(\"N/A\")\n",
    "        print(str(manip_count) + \" - N/A\")\n",
    "        \n",
    "player_df = pd.merge(player_df, college_df, right_index=True, left_index=True)\n",
    "player_df.rename(index=str, columns={\"0\": \"College\"})\n",
    "player_df.columns=['blank', 'player_id', 'name', 'college']\n",
    "player_df = player_df.iloc[:,[1,2,3]]\n",
    "player_df.to_csv(\"player_college.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in our data collection process was to calculate the amount of fantasy points \n",
    "generated by each player. To do this we made a dictionary of each stat category (passing yards, \n",
    "rushing yards, etc..) and assigned a multiplier value for each one. We looped through each \n",
    "row in our offensive_players dataframe, multiplying each column by the appropriate value. \n",
    "This was an awesome script we called 'apply_fantasy.ipynb'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV = \"Offense_Only.csv\"\n",
    "\n",
    "with open(CSV, \"r\") as inbox:\n",
    "    # DATA is the original data\n",
    "    DATA = pandas.read_csv(inbox)\n",
    "\n",
    "RENAME = {\"P_Yards\": \"passing yards\",\n",
    "          \"P_TD\": \"passing touchdowns\", \n",
    "          \"P_Int\": \"passing interceptions\", \n",
    "          \"Rush_Yards\": \"rushing yards\",\n",
    "          \"Rush_TD\": \"rushing touchdowns\",\n",
    "          \"Fumbles\": \"fumbles\",\n",
    "          \"Rec_Yards\": \"receiving yards\",\n",
    "          \"Rec_TD\": \"receiving touchdowns\",\n",
    "          \"Receptions\": \"receptions\",\n",
    "          \"Kick_Returns\": \"kick returns\",\n",
    "          \"Punt_Returns\": \"punt returns\"\n",
    "         }    \n",
    "result = DATA.rename(columns = RENAME)    \n",
    "LOCATAION = \"fantasy_stats_final.csv\"\n",
    "\n",
    "END_POINTS = {\n",
    "    \"passing yards\": 0.04,\n",
    "    \"passing touchdowns\": 4,\n",
    "    \"passing interceptions\": -2,\n",
    "    \"rushing yards\": 0.1,\n",
    "    \"rushing touchdowns\": 6,\n",
    "    \"receiving yards\": 0.1,\n",
    "    \"receiving touchdowns\": 6,\n",
    "    \"fumbles\": -2,\n",
    "    \"kick returns\": 6,\n",
    "    \"punt returns\": 6\n",
    "}\n",
    "\n",
    "# result = pandas.DataFrame()\n",
    "\n",
    "for each in END_POINTS:\n",
    "    result[each] = result[each].map(lambda x: x*END_POINTS[each])\n",
    "result.drop(result.columns[0], axis = 1, inplace = True)\n",
    "with open(LOCATAION, \"w\") as outbox:\n",
    "    result.to_csv(outbox, index = False)\n",
    "    \n",
    "    \n",
    "CSV = \"fantasy_stats_final.csv\"\n",
    "\n",
    "with open(CSV, \"r\") as inbox:\n",
    "    # DATA is the original data\n",
    "    data = pandas.read_csv(inbox)\n",
    "    \n",
    "totals_list = []\n",
    "for x in range(0,len(data)):  \n",
    "    totals = data.iloc[x,[6,7,8,9,10,11,12,13,15,16]].sum()\n",
    "    totals_list.append(totals)\n",
    "totals_df = pd.DataFrame(totals_list)\n",
    "data_df = pd.DataFrame(data)\n",
    "totals_df.columns = ['total annual points']\n",
    "final_with_totals = pd.concat([data_df, totals_df], axis=1)\n",
    "final_with_totals.to_csv('final_with_totals.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we had 2 data sets, 1 contained all of the stats information, the other contained\n",
    "all the college data for each player. Using a final script we merged the two datasets together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CSV = \"final_with_totals.csv\"\n",
    "\n",
    "with open(CSV, \"r\") as inbox:\n",
    "    # DATA is the original data\n",
    "    data = pandas.read_csv(inbox)\n",
    "\n",
    "merged = data.merge(data2, left_on=['Player_Id'], right_on=['player_id'], how='left')\n",
    "merged.to_csv('actually_final_i_promise.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
